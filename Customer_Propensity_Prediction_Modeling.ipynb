{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Product Propensity Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion + Exploration + Splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Ingestion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product = pd.read_excel('product_sales_v3.xlsx', sheet_name='Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Exploration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We perform the below exploratory data analysis to get a basic undersatnding of the data, without letting it leak into our predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are no duplicate rows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product[product.duplicated()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We convert the order_day field to a datetime:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#convert the order_day to a datetime MM/DD/YYYY format\n",
    "product.order_day = pd.to_datetime(product.order_day, format='%m/%d/%Y')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance rates per segment:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product.groupby('segment').agg({'accept': 'mean'}).plot.bar(title='acceptance rate per segment')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance rate per marketing area:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product.groupby('dma').agg({'accept': 'mean'}).plot.bar(title='acceptance rate per dma')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance rate per year:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#acceptance trends by year\n",
    "product[['accept']].groupby([product['order_day'].astype(str).str[:-6]]).mean().plot.bar(\n",
    "    title='acceptance rate per year')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product[['order_day', 'accept']].groupby([product['order_day'].astype(str).str[:-6]]).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance rate per month:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#acceptance trends by month\n",
    "product[['accept']].groupby([product['order_day'].astype(str).str[5:7]]).mean().plot.bar(\n",
    "    title='acceptance rate per month')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance rate over continuous time:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#acceptance trends by month/year\n",
    "product[['accept']].groupby([product['order_day'].astype(str).str[:7]]).mean().plot.line(\n",
    "    title='acceptance rate per month-year')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance volume per year:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product[['accept']].groupby([product['order_day'].astype(str).str[:-6]]).count().plot.bar(\n",
    "    title='acceptance volume per year')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the acceptance volume per month:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product[['accept']].groupby([product['order_day'].astype(str).str[5:7]]).count().plot.bar(\n",
    "    title='acceptance volume per month')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We visualize the top 10 days with the most acceptances:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product[['accept']].groupby([product['order_day'].astype(str).str[:10]]).count().sort_values(by='accept',                                                                                    ascending=False).head(\n",
    "    10).plot.bar(title='acceptance volume - top 10 days')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train/Test Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time-based splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create time-based split, with first 75% in train data, last 25% in test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "product.sort_values(by='order_day', inplace=True)  #use for time series sort to sort by date before splitting\n",
    "product_X_data = product.drop('accept', axis=1)  #remove the target variable from the X data\n",
    "product_y_data = product[['accept']]\n",
    "product_x_train, product_x_test, product_y_train, product_y_test = train_test_split(product_X_data, product_y_data, test_size=.25, random_state=100, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check distribution of acceptance rate for above splitting approach:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product_y_train['accept'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product_y_test['accept'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check distribution of order day for above splitting approach:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product_x_train['order_day'].sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product_x_test['order_day'].sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing Pipeline Creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning + Feature Engineering Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert data types to predefined dictionary of field:dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_dtypes(df, data_type_transformation):\n",
    "    \"\"\"convert data types to predefined dictionary of field:dtype\"\"\"\n",
    "    df = df.astype(data_type_transformation)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert Y/NaN field to Y/N - this is meant for the tos_flg field"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_nan(data, col):\n",
    "    \"\"\"convert NaN's to N for certain Y/N columns\"\"\"\n",
    "    data[col] = data[col].fillna('N')\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert numerical blank field values to 0 - this is intended for the deposit_onhand_amt field"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_blank(data, col):\n",
    "    \"\"\"convert blanks to 0 for certain numeric columns\"\"\"\n",
    "    data[col] = data[col].fillna(0)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep the left n characters of a columns - this is intended for the zipcode field"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#get the left 5 digits of zipcode\n",
    "def left_strip(data, col, num_char):\n",
    "    \"\"\"keep only the left n of characters\"\"\"\n",
    "    data[col] = data[col].astype('str').str[:num_char]\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert zipcode to latitude/longitude value - this is intended for the zipcode field"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def zip_to_latlong(df, left_col, right_col, mapping_file):\n",
    "    \"\"\"convert zipcode to latitude/longitude\"\"\"\n",
    "    zip_code_map = pd.read_csv(mapping_file)\n",
    "    zip_code_map[right_col] = zip_code_map[right_col].astype('str')\n",
    "    data_merged = pd.merge(df, zip_code_map, left_on=left_col, right_on=right_col, how='left')\n",
    "    data_merged = data_merged.drop(left_col, axis=1)\n",
    "    data_merged = data_merged.drop(right_col, axis=1)\n",
    "    return data_merged"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert true/false to 1/0 - this is meant for boolean columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_boolean(data):\n",
    "    \"\"\"convert True/False to 1/0 for boolean columns\"\"\"\n",
    "    for col in data.select_dtypes(include=[bool]):\n",
    "        data[col].replace(True, 1, inplace=True)\n",
    "        data[col].replace(False, 0, inplace=True)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert Y/N columns to 1/0 - this is meant for boolean columns that are represented as Y/N strings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#convert Y/N columns to 1/0, hardcode columns to avoid picking up true Y/N non-boolean values?\n",
    "def replace_yn(data):\n",
    "    \"\"\"convert Y/N to 1/0 for Y/N columns\"\"\"\n",
    "    for col in data.select_dtypes(include=[object]):\n",
    "        data[col].replace('Y', 1, inplace=True)\n",
    "        data[col].replace('N', 0, inplace=True)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert non-numeric values to NaN in a numeric column - this is meant for the term_length column, where there are several term length values that are non-numeric in the dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def replace_non_numeric(data, col):\n",
    "    '''convert non-numeric data in a numeric field to NaN'''\n",
    "    data[col] = data[col].astype('str').replace(r'^[^0-9]', np.nan, regex=True).astype('float')\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert order_day to seasons - this is based on subjective seasonal patterns. We will define 3 seasons: cool = Dec-Feb; hot = Jun-Sept; average = Mar-May, Oct-Nov."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_seasons(df, input_col, output_col, month_bins, season_labels):\n",
    "    \"\"\"create seasons based on predefined season labels\"\"\"\n",
    "    df[output_col] = pd.cut(df[input_col].astype(str).str[5:7].astype(int), bins=month_bins, labels=season_labels,\n",
    "                            ordered=False)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop columns if the percentage of null values in the column exceeds a certain threshold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_null_value_columns_threshold(data, null_value_percentage_table, threshold=1.00):\n",
    "    \"\"\"remove columns if more than threshold % of its values are null\"\"\"\n",
    "    null_value_fields = null_value_percentage_table[(null_value_percentage_table > threshold)].index\n",
    "    df = data.drop(null_value_fields, axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop user-defined column(s) based on column name(s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_null_value_columns_fields(data, fields_to_drop):\n",
    "    \"\"\"remove columns if the column is in a specific list of fields\"\"\"\n",
    "    df = data.drop(fields_to_drop, axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform cube root transformation. Based on some previous histogram plotting in our sandbox trial and error testing, the curr_usage field was skewed. Transforming this field by taking its cube root normalizes its data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cube_rt_transform(data, input_col, output_col):\n",
    "    \"\"\"create a cube root transformation output column from a certain input column\"\"\"\n",
    "    data[output_col] = np.cbrt(data[input_col])\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bin columns from many numerical values to a few higher-level categorized levels. This was intended for the home_value and term_length columns, as these had very granular data, and for home_value was heavily imputed so those values themselves may not have meant as much as the strata of that they are in conveyed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def binning_transform(data, input_col, output_col, bin_cutoffs, bin_labels):\n",
    "    \"\"\"create a cube root transformation column an the output of a defined input column\"\"\"\n",
    "    data[output_col] = pd.cut(data[input_col], bins=bin_cutoffs, labels=bin_labels)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning + Feature Engineering Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When running this final pipeline, we only use the relevant transformations. We removed transformations that were not useful. The sandbox file has the commented out versions of other transformations that were not useful.\n",
    "\n",
    "Here we define the final parameters that will be used in the data cleaning pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "drop_cols = ['order_day', 'customer_id']\n",
    "month_bins = [0, 2, 5, 9, 11, 12]\n",
    "season_labels = ['cool', 'average', 'hot', 'average', 'cool']\n",
    "null_value_percentage_tbl = product_x_train.isnull().sum() / product_x_train.shape[0]\n",
    "term_length_bins = [0, 12, 24, 36, 48, 60, 99999999]\n",
    "term_length_bin_labels = ['up_to_1_year', '1-2_years', '2-3_years', '3-4_years', '4-5_years', '5+_years']\n",
    "data_type_transformations = {'order_day': 'datetime64[ns]', 'tos_flg': object, 'disconotice_flg': bool,\n",
    "                             'oam_activelogin_cnt': np.int64, 'term_length': object, 'called_numcalls_cnt': np.int64,\n",
    "                             'latefee_flg': bool, 'dwelling_type_cd': object, 'curr_usage': np.float64,\n",
    "                             'product_type_cd': object, 'pool': object, 'automatic_payment_flg': bool,\n",
    "                             'weblog_flg': bool, 'deposit_onhand_amt': np.float64, 'ebill_enroll_flag': bool,\n",
    "                             'called_flg': bool, 'oam_flg': bool, 'sap_productname': object, 'numweblog_cnt': np.int64,\n",
    "                             'disconnects_flg': bool, 'load_profile': object, 'city': object, 'zipcode': object,\n",
    "                             'home_value': np.float64, 'county': object, 'tdsp': object, 'dma': object,\n",
    "                             'ev_driver': bool, 'segment': np.int64, 'customer_id': object, 'meter_id': object}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we take the above parameters and apply them to the data via the pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#run the pipeline - in this final version, only use the relevant values, cleared out irrelevant transformations\n",
    "data_cleaning_pipeline = Pipeline(steps=[\n",
    "    (\"convert_dtypes\",\n",
    "     FunctionTransformer(convert_dtypes, kw_args={\"data_type_transformation\": data_type_transformations})),\n",
    "    (\"replace_NAs\", FunctionTransformer(replace_nan, kw_args={\"col\": 'tos_flg'})),\n",
    "    (\"replace_blanks\", FunctionTransformer(replace_blank, kw_args={\"col\": 'deposit_onhand_amt'})),\n",
    "    (\"left_strip\", FunctionTransformer(left_strip, kw_args={\"col\": 'zipcode', \"num_char\": 5})),\n",
    "    (\"replace_boolean\", FunctionTransformer(replace_boolean)),\n",
    "    (\"replace_yn\", FunctionTransformer(replace_yn)),\n",
    "    (\"replace_non_numeric\", FunctionTransformer(replace_non_numeric, kw_args={\"col\": 'term_length'})),\n",
    "    (\"create_seasons\", FunctionTransformer(create_seasons, kw_args={\"input_col\": 'order_day', \"output_col\": 'seasons',\n",
    "                                                                    \"month_bins\": month_bins,\n",
    "                                                                    \"season_labels\": season_labels})),\n",
    "    (\"drop_null_value_columns_thresh\", FunctionTransformer(drop_null_value_columns_threshold, kw_args={\n",
    "        \"null_value_percentage_table\": null_value_percentage_tbl, \"threshold\": 0.50})),\n",
    "    (\"drop_null_value_columns_dropcols\",\n",
    "     FunctionTransformer(drop_null_value_columns_fields, kw_args={\"fields_to_drop\": drop_cols})),\n",
    "    (\"cube_rt_transform\",\n",
    "     FunctionTransformer(cube_rt_transform, kw_args={\"input_col\": 'curr_usage', \"output_col\": \"curr_usage_cbrt\"})),\n",
    "    (\"binning_transform_term_length\", FunctionTransformer(binning_transform, kw_args={'input_col': 'term_length',\n",
    "                                                                                      'output_col': 'term_length_binned',\n",
    "                                                                                      'bin_cutoffs': term_length_bins,\n",
    "                                                                                      'bin_labels': term_length_bin_labels}))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imputation and Scaling Pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we impute and scale the numeric data and impute and one hot encode the categorical data. These final imputation, scaling, and OHE strategies are based on previous experiments:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a feature engineering pipeline to impute and scale data\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "feature_engineering_col_trans = ColumnTransformer(transformers=[\n",
    "    ('numeric', Pipeline([\n",
    "        ('impute_mean', SimpleImputer(strategy=\"mean\")),\n",
    "        ('scaler_minmax', MinMaxScaler())\n",
    "    ]), make_column_selector(dtype_include=np.number)),\n",
    "    ('categorical', Pipeline([\n",
    "        ('impute_mode', SimpleImputer(strategy=\"most_frequent\")),\n",
    "        ('OHE_500', OneHotEncoder(min_frequency=500, handle_unknown=\"ignore\"))\n",
    "    ]), make_column_selector(dtype_include=object))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We combine the above data cleaning, feature engineering, imputation, scaling pipelines into 1 combined preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    ('data_cleaning_pipeline', data_cleaning_pipeline),\n",
    "    ('feature_engineering_pipeline', feature_engineering_col_trans)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Inclusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function to combine the above preprocessing pipeline and append a modeling step to the end of it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_pipeline_creation(preprocessing_pipe, model):\n",
    "    '''combine the above preprocessing pipeline and append a modeling step to the end of it'''\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessing', preprocessing_pipe),\n",
    "        ('model', model)\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that performs hyperparameter tuning via randomized search to maximize the ROC AUC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(model_pipe, hyperparameter_search_space, iterations, cv_folds, scoring_rules):\n",
    "    '''performs hyperparameter tuning via randomized search to maximize the ROC AUC'''\n",
    "    return RandomizedSearchCV(model_pipe, param_distributions=hyperparameter_search_space, n_iter=iterations,\n",
    "                              cv=cv_folds,\n",
    "                              scoring=scoring_rules, random_state=0, refit='roc_auc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Refitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that takes the tuned-hyperparameter model and re-fits the train data on it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_fitting(pipe_hyper_tuned, x_train_data, y_train_data):\n",
    "    '''takes the tuned-hyperparameter model and re-fits the train data on it'''\n",
    "    return pipe_hyper_tuned.fit(x_train_data, y_train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Confusion Matrix Creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that takes the tuned model and predicts it using the test data, and then creates the confusion matrix from that test data predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "\n",
    "def create_confusion_matrix(pipe_hyper_tuned, x_test_data, y_test_data):\n",
    "    '''takes the tuned model and predicts it using the test data, and then creates the confusion matrix from that test data'''\n",
    "    model_cm = confusion_matrix(y_test_data, pipe_hyper_tuned.predict(x_test_data), labels=pipe_hyper_tuned.classes_,\n",
    "                                normalize='all')\n",
    "    model_cm_disp = ConfusionMatrixDisplay(confusion_matrix=model_cm, display_labels=pipe_hyper_tuned.classes_)\n",
    "    model_cm_disp.plot(cmap='Blues')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot ROC AUC curve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that takes the tuned model and creates the ROC AUC curve from those test data predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "def plot_roc_auc_curve(pipe_hyper_tuned, x_test_data, y_test_data):\n",
    "    '''takes the tuned model and creates the ROC AUC curve from those test data'''\n",
    "    return RocCurveDisplay.from_estimator(pipe_hyper_tuned, x_test_data, y_test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Scoring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that takes the optimized model and provides its best AUC score on the train and test data, the best hyperparameters, and the best F2 score from that test data predictions, as well as a detailed dataframe showing the model performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_scoring(pipe_hyper_tuned, x_test_data, y_test_data):\n",
    "    '''takes the optimized model and provides its best AUC score on the train and test data, the best hyperparameters, and the best F2 score from that test data predictions, as well as a detailed dataframe showing the model performance'''\n",
    "    print(\"Best ROC AUC Score of train set: \" + str(pipe_hyper_tuned.best_score_))\n",
    "    print(\"Best parameter set: \" + str(pipe_hyper_tuned.best_params_))\n",
    "    print(\"Test ROC AUC Score: \" + str(pipe_hyper_tuned.score(x_test_data, y_test_data)))\n",
    "    print(\"Test F2 Score: \" + str(fbeta_score(y_test_data, pipe_hyper_tuned.predict(x_test_data), beta=2)))\n",
    "    return pd.DataFrame(pipe_hyper_tuned.cv_results_).sort_values(by='rank_test_roc_auc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Full model fitting and scoring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function that takes all the above modeling pipeline steps, and puts it together in one consolidated pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "def modeling_pipeline_full(preprocessing_pipe, model_name, model_param_dist, iterations, cv_folds, scoring_rules,\n",
    "                           x_train_data, y_train_data, x_test_data, y_test_data):\n",
    "    '''takes all the above modeling pipeline steps, and puts it together in one consolidated pipeline'''\n",
    "    model_pipeline_function = model_pipeline_creation(preprocessing_pipe, model_name)\n",
    "    model_pipeline_hyperparameter_tuned = hyperparameter_tuning(model_pipeline_function, model_param_dist, iterations,\n",
    "                                                                cv_folds, scoring_rules)\n",
    "    model_model_train = model_fitting(model_pipeline_hyperparameter_tuned, x_train_data, y_train_data)\n",
    "    create_confusion_matrix(model_pipeline_hyperparameter_tuned, x_test_data, y_test_data)\n",
    "    plot_roc_auc_curve(model_pipeline_hyperparameter_tuned, product_x_test, product_y_test)\n",
    "    return model_scoring(model_pipeline_hyperparameter_tuned, x_test_data, y_test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling Experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the scoring metrics we'll calculate for each model. We will score each model on its ROC AUC and its F2 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoring = {\"roc_auc\": 'roc_auc', 'f2_scorer': make_scorer(fbeta_score, beta=2)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "log_reg_param_dist = {'model__C': randint(low=1, high=10),\n",
    "                      'model__class_weight': [None, 'balanced']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, log_reg, log_reg_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_param_dist = {'model__n_neighbors': randint(low=2, high=10)}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, knn, knn_param_dist, 2, 5, scoring, product_x_train, product_y_train,\n",
    "                       product_x_test, product_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SGD Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "\n",
    "sgdc = SGDClassifier(max_iter=10, tol=1e-1, random_state=0)\n",
    "\n",
    "sgdc_param_dist = {'model__alpha': (random.random() / 10000, random.random() / 1000),\n",
    "                   'model__class_weight': [None, 'balanced']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, sgdc, sgdc_param_dist, 2, 5, scoring, product_x_train, product_y_train,\n",
    "                       product_x_test, product_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decision Tree"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dtree_param_dist = {'model__max_features': randint(low=10, high=100),\n",
    "                    'model__max_depth': randint(low=2, high=20),\n",
    "                    'model__class_weight': [None, 'balanced']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, dtree, dtree_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_param_dist = {'model__n_estimators': randint(low=50, high=500),  #165\n",
    "                 'model__max_features': randint(low=10, high=50),  #30\n",
    "                 'model__max_depth': randint(low=2, high=20),  #11\n",
    "                 'model__class_weight': [None, 'balanced']}  #balanced\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, rf, rf_param_dist, 2, 5, scoring, product_x_train, product_y_train,\n",
    "                       product_x_test, product_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ExtraTrees"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xtree = ExtraTreesClassifier(random_state=0)\n",
    "\n",
    "xtree_param_dist = {'model__n_estimators': randint(low=50, high=500),\n",
    "                    'model__max_features': randint(low=5, high=50),\n",
    "                    'model__max_depth': randint(low=2, high=20),\n",
    "                    'model__class_weight': [None, 'balanced']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, xtree, xtree_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LightGBM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/prashant111/lightgbm-classifier-in-python\n",
    "import lightgbm as lgbm\n",
    "\n",
    "lightgbm = lgbm.LGBMClassifier(random_state=0)\n",
    "\n",
    "lightgbm_param_dist = {'model__max_depth': randint(low=1, high=10),  #6, 4\n",
    "                       'model__num_leaves': randint(low=50, high=250),  #117, 59, 137\n",
    "                       'model__n_estimators': randint(low=50, high=250),  #245, 86\n",
    "                       'model__class_weight': [None, 'balanced']}  #balanced\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, lightgbm, lightgbm_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Voting Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=0)),\n",
    "        ('rf', RandomForestClassifier(random_state=0)),\n",
    "        ('lgbm', lgbm.LGBMClassifier(random_state=0))\n",
    "    ], voting='soft')\n",
    "\n",
    "voting_param_dist = {'model__lr__C': randint(low=1, high=10),  #2,3 optimal\n",
    "                     'model__rf__n_estimators': randint(low=50, high=500),  #387\n",
    "                     'model__rf__max_features': randint(low=50, high=200),  #98, 97 optimal\n",
    "                     'model__rf__max_depth': randint(low=2, high=20),  #16,6 optimal\n",
    "                     'model__lgbm__max_depth': randint(low=5, high=20),  #13,12 optimal\n",
    "                     'model__lgbm__num_leaves': randint(low=50, high=250),  #190, 71 optimal\n",
    "                     'model__lgbm__n_estimators': randint(low=50, high=250),  #138\n",
    "                     'model__lr__class_weight': [None, 'balanced'],  #none\n",
    "                     'model__rf__class_weight': [None, 'balanced'],  #balanced\n",
    "                     'model__lgbm__class_weight': [None, 'balanced']}  #none\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, voting, voting_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Stacking Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=0)),\n",
    "        ('rf', RandomForestClassifier(random_state=0)),\n",
    "        ('lgbm', lgbm.LGBMClassifier(random_state=0))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=0),\n",
    "    cv=2  # number of cross-validation folds\n",
    ")\n",
    "\n",
    "stacking_param_dist = {'model__lr__C': randint(low=1, high=10),  #4\n",
    "                       'model__rf__n_estimators': randint(low=50, high=500),  #120\n",
    "                       'model__rf__max_features': randint(low=10, high=100),  #19\n",
    "                       'model__rf__max_depth': randint(low=2, high=20),  #9\n",
    "                       'model__lgbm__max_depth': randint(low=1, high=10),  #6\n",
    "                       'model__lgbm__num_leaves': randint(low=50, high=250),  #117\n",
    "                       'model__lgbm__n_estimators': randint(low=50, high=350),\n",
    "                       'model__lr__class_weight': [None, 'balanced'],\n",
    "                       'model__rf__class_weight': [None, 'balanced'],\n",
    "                       'model__lgbm__class_weight': [None, 'balanced']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, stacking, stacking_param_dist, 2, 5, scoring, product_x_train,\n",
    "                       product_y_train, product_x_test, product_y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neural Network - MLP Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run model through the full preprocessing/modeling pipeline, using the defined hyperparameter search space and 2 iterations of 5-fold cross-validation. This will output the model scoring as well as the confusion matrix and AUC plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter=50, random_state=0)\n",
    "\n",
    "mlp_param_dist = {'model__hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,), (10, 30, 10), (20,)],\n",
    "                  'model__activation': ['tanh'],  #tanh is consistently better than relu\n",
    "                  'model__alpha': [0.0001, 0.05],\n",
    "                  'model__learning_rate': ['constant', 'adaptive']}\n",
    "\n",
    "modeling_pipeline_full(preprocessing_pipeline, mlp, mlp_param_dist, 2, 5, scoring, product_x_train, product_y_train,\n",
    "                       product_x_test, product_y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## True Test Data Probability Predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict probabilities using winning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "#create the test data from the to be provided true test data\n",
    "product_true_test_x_data = pd.read_csv('product_sales_test.csv')\n",
    "\n",
    "\n",
    "def modeling_pipeline_test_predictions(preprocessing_pipe, model_name, model_param_dist, iterations, cv_folds,\n",
    "                                       scoring_rules, x_train_data, y_train_data, x_test_data_true):\n",
    "    '''re-run the preprocessing/modeling pipeline, but predict the model probabilities'''\n",
    "    model_pipeline_function = model_pipeline_creation(preprocessing_pipe, model_name)\n",
    "    model_pipeline_hyperparameter_tuned = hyperparameter_tuning(model_pipeline_function, model_param_dist, iterations,\n",
    "                                                                cv_folds, scoring_rules)\n",
    "    model_model_train = model_fitting(model_pipeline_hyperparameter_tuned, x_train_data, y_train_data)\n",
    "    model_model_predictions = model_pipeline_hyperparameter_tuned.predict_proba(x_test_data_true)\n",
    "    return model_model_predictions\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the above updated function, apply it to the random forest model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scoring = {\"roc_auc\": 'roc_auc', 'f2_scorer': make_scorer(fbeta_score, beta=2)}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_param_dist = {'model__n_estimators': randint(low=50, high=500),  #165\n",
    "                 'model__max_features': randint(low=10, high=50),  #30\n",
    "                 'model__max_depth': randint(low=2, high=20),  #11\n",
    "                 'model__class_weight': [None, 'balanced']}  #balanced\n",
    "\n",
    "#create the model predictions for the best model (RF), 3 iterations of 5-fold CV\n",
    "best_model_predictions = modeling_pipeline_test_predictions(preprocessing_pipeline, rf, rf_param_dist, 2, 5,\n",
    "                                                            scoring, product_x_train, product_y_train,\n",
    "                                                            product_true_test_x_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check to make sure number of test rows == the number of prediction rows (yes, both are 20,328 rows)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert (product_true_test_x_data.shape[0] == len(best_model_predictions[:, 1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We check this model's predictions. The left column is probability of a row being classified as 0, right column is probability of classified as 1. To get the probabilities of acceptance we take just column 1. These are the final prediction probabilities we will output to csv and send:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the distribution of acceptance prediction probabilities. As expected, most probabilities are closer to 0:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(best_model_predictions[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also plot the distribution of the best model predictions on the true test set (converted to 0 and 1) and it is a large majority 0's, as expected:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(best_model_predictions, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}